{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Dataframes\n",
    "\n",
    "In this lesson we will continue working with pandas DataFrames, and explore some more complex DataFrame manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of values for names column.\n",
    "\n",
    "students = ['Sally', 'Jane', 'Suzie', 'Billy', 'Ada', 'John', 'Thomas',\n",
    "            'Marie', 'Albert', 'Richard', 'Isaac', 'Alan']\n",
    "\n",
    "# Randomly generate arrays of scores for each student for each subject.\n",
    "# Note that all the values need to have the same length here.\n",
    "\n",
    "math_grades = np.random.randint(low=60, high=100, size=len(students))\n",
    "english_grades = np.random.randint(low=60, high=100, size=len(students))\n",
    "reading_grades = np.random.randint(low=60, high=100, size=len(students))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the DataFrame using the above lists and arrays.\n",
    "\n",
    "df = pd.DataFrame({'name': students,\n",
    "                   'math': math_grades,\n",
    "                   'english': english_grades,\n",
    "                   'reading': reading_grades,\n",
    "                   'classroom': np.random.choice(['A', 'B'], len(students))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Lists and Dictionaries\n",
    "\n",
    "There are several ways to create dataframes, we've already seen how we can create a dataframe from a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in the passed dictionary will be the column names, and the values are the data points that make up each column.\n",
    "\n",
    "We can also create dataframes from a 2d data structure, either a numpy array or a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[1, 2, 3], [4, 5, 6 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "pd.DataFrame(array, columns=['this', 'that', 'the other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that we had to specify the names of the columns ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From PyDataset.\n",
    "\n",
    "In some of the exercises, you'll need to load several datasets using the `pydataset` library. (If you get an error when trying to run the import below, use `pip` to install the `pydataset` package.) When the instructions say to load a dataset from PyDataset, you will need to do the following:\n",
    "\n",
    "The following import is necessary to access PyDataset datasets:\n",
    "```python\n",
    "from pydataset import data\n",
    "```\n",
    "\n",
    "Running this code snippet will show you the valuable information doc on the dataset:\n",
    "```python\n",
    "data(df_string_name, show_doc=True)\n",
    "```\n",
    "\n",
    "Running this code snippet will load the dataset for use as a pandas DataFrame:\n",
    "```python\n",
    "df = data(df_string_name)\n",
    "```\n",
    "\n",
    "There are 757 available datasets using pydataset. Running the following code snippet in a cell will return a DataFrame with all of your options:\n",
    "```python\n",
    "data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and store it in the variable mpg.\n",
    "\n",
    "from pydataset import data\n",
    "mpg = data('mpg')\n",
    "mpg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the documentation for the dataset, and any pydata dataset, by setting show_doc to True. This outputs valuable context for your dataset.\n",
    "\n",
    "```python\n",
    "data('mpg', show_doc=True) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From SQL\n",
    "\n",
    "We can use the `read_sql` method to create a dataframe based on the results of a SQL query. To do this, we need to tell pandas how to connect to the database we are querying. The way we communicate this to pandas is with a specially formatted *connection string*.\n",
    "\n",
    "In addition, whenever we want to connect to a database from our python code (other programming languages are similar), we will need a **driver**, a bit of software that handles the details of the database connection.\n",
    "\n",
    "In order to connect to mysql, we'll install the `pymysql` driver packages:\n",
    "\n",
    "`python -m pip install pymysql`\n",
    "\n",
    "Once those are installed, we can create the connection string. In general, database connection urls will have this format:\n",
    "\n",
    "```python\n",
    "protocol://[user[:password]@]hostname/[database_name]\n",
    "```\n",
    "\n",
    "Here's an example of what one would look like:\n",
    "\n",
    "```python\n",
    "mysql+pymysql://codeup:p@assw0rd@123.123.123.123/some_db\n",
    "```\n",
    "\n",
    "Another thing we need to consider is that we don't want to publish our database credentials to github, however, we will need access to these values in our code in order to create the connection string defined above.\n",
    "\n",
    "In order to accomplish this, we can define several variables in a file named `env.py` that contain the sensitive data, add `env.py` to our `.gitignore` file, and then import those values into another script. \n",
    "\n",
    "**Be 100% sure to add `env.py` to this specific repository's `.gitignore` file, even and especially, if you have already added `env.py` to your global .gitignore file. This will protect the env file for people who clone this project (like collaborators)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_sql(query, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import host, user, password\n",
    "\n",
    "url = f'mysql+pymysql://{user}:{password}@{host}/employees'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this url is defined, we can use it with the `read_sql` function to have pandas treat the results of a SQL query as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT * FROM employees LIMIT 5 OFFSET 50', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to have longer SQL queries that we want to read into python, and an example of how we might break a query into several lines is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT\n",
    "    emp_no,\n",
    "    first_name,\n",
    "    last_name\n",
    "FROM employees\n",
    "WHERE gender = 'F'\n",
    "LIMIT 100\n",
    "'''\n",
    "\n",
    "employees = pd.read_sql(sql, url)\n",
    "employees.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!danger \"Passwords and Sensitive Information\"\n",
    "    Don't add and commit files with passwords or other sensitive information in them to a git repository!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "    t.title as title,\n",
    "    d.dept_name as dept_name\n",
    "FROM titles t\n",
    "JOIN dept_emp USING (emp_no)\n",
    "JOIN departments d USING (dept_no)\n",
    "LIMIT 100\n",
    "'''\n",
    "\n",
    "title_dept = pd.read_sql(query, url)\n",
    "title_dept.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises I\n",
    "\n",
    "Run `python -m pip install pymysql` from your terminal to install pymysql.\n",
    "\n",
    "Create a notebook or python script named `advanced_dataframes` to do your work in for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run `python -m pip install pymysql` from your terminal to install the mysql client (any folder is fine)\n",
    "1. cd into your exercises folder for this module and run `echo env.py >> .gitignore`\n",
    "1. Create a function named `get_db_url`. It should accept a username, hostname, password, and database name and return a url connection string formatted like in the example at the start of this lesson.\n",
    "\n",
    "2. Use your function to obtain a connection to the `employees` database.\n",
    "\n",
    "3. Once you have successfully run a query:\n",
    "\n",
    "    a. Intentionally make a typo in the database url. What kind of error message do you see?\n",
    "    \n",
    "    b. Intentionally make an error in your SQL query. What does the error message look like?\n",
    "\n",
    "4. Read the `employees` and `titles` tables into two separate DataFrames.\n",
    "\n",
    "5. How many rows and columns do you have in each DataFrame? Is that what you expected?\n",
    "\n",
    "6. Display the summary statistics for each DataFrame.\n",
    "\n",
    "7. How many unique titles are in the `titles` DataFrame?\n",
    "\n",
    "8. What is the oldest date in the `to_date` column? \n",
    "\n",
    "9. What is the most recent date in the `to_date` column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Subsetting\n",
    "\n",
    "Like the pandas Series object, the pandas DataFrame object supports both position- and label-based indexing using the indexing operator `[]`.\n",
    "\n",
    "I will demonstrate concrete examples of indexing using the indexing operator `[]` alone and with the `.loc` and `.iloc` attributes below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `[]`\n",
    "\n",
    "I can pass a list of columns from a DataFrame to the indexing operator (aka bracket notation) to return a subset of my original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose only two columns for my subset.\n",
    "\n",
    "df[['name', 'classroom']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can pass a boolean Series to the indexing operator as a selector.\n",
    "\n",
    "bools = df.name.str.startswith('A')\n",
    "bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[bools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.loc`\n",
    "\n",
    "We can use the `.loc` attribute to select specific rows AND columns by index label. The index label can be a number, but it can also be a string label. This method offers a lot of flexibility! **The .loc attribute's indexing is inclusive and uses an index label, not position.**\n",
    "\n",
    "```python\n",
    "df.loc[row_indexer, column_indexer]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the rows and a subset of columns; notice the inclusive behavior of the indexing.\n",
    "\n",
    "df.loc[:, 'math':'reading']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.iloc`\n",
    "\n",
    "We can use the `.iloc` attribute to select specific rows and colums by index position. `.iloc` does not accept a boolean Series as a selector like `.loc` does. **It takes in integers representing index position and is NOT inclusive.**\n",
    "\n",
    "```python\n",
    "df.iloc[row_indexer, column_indexer]\n",
    "```\n",
    "\n",
    "We can select rows by integer position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the exclusive behavior of the indexing.\n",
    "\n",
    "df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify which columns we want to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the first 3 rows (everything up to but not including the index of 3), and the second and third columns (starting from the index of 1 up to but not including the index of 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.agg`\n",
    "\n",
    "The `.agg` method lets us specify a way to aggregate a series of numerical values. We pass an aggregate function or list of functions to the method that we want applied to a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.math.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.math.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reading.agg('min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While on the surface this seems pretty simple, `.agg` is capable of providing more detailed aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['english', 'reading', 'math']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['english', 'reading', 'math']].agg(['mean', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.groupby`\n",
    "\n",
    "The `.groupby` method is used to create a grouped object, which we can then apply an aggregation on. For example, if we wanted to know the highest math grade from each classroom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"classroom\", \"math\", \"reading\", \"english\"]].groupby(\"classroom\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('classroom').math.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.agg` here to, to see multiple aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('classroom').math.agg(['min', 'mean', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group by multiple columns as well. To demonstrate, we'll create a boolean column named `passing_math`, then group by the combination of our new feature, `passing_math`, and the classroom and calculate the average reading grade and the number of individuals in each subgroup. \n",
    "\n",
    "Let's break this problem down and code it step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `np.where`\n",
    "\n",
    "First, we can create the new `passing_math` column using a handy NumPy function called `np.where`. It will allow us to base the new column values on whether the values in an existing column, `math`, meet a condition.\n",
    "\n",
    "```python\n",
    "np.where(condition, this_where_True, this_where_False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column based on an existing column.\n",
    "\n",
    "df['passing_math'] = np.where(df.math < 70, 'failing', 'passing')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will group by the `passing_math` and `classroom` columns and use the `.agg` method to calculate the average reading grade and the number of students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_groups = df.groupby(['passing_math', 'classroom']).reading.agg(['mean', 'count'])\n",
    "grade_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can even clean up my columns to make my calculations clearer.\n",
    "\n",
    "grade_groups.columns = ['avg_reading_grade', 'count_of_students']\n",
    "grade_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "We can interpret this output as there being 2 students failing math in classroom A with an average reading grade of 87, 6 students passing math in classroom A with an average reading grade of 87.16, and 4 students passing math in classroom B with an average reading grade of 85.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining\n",
    "\n",
    "Pandas provides several ways to combine dataframes together. We will look at two of them below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.concat`\n",
    "\n",
    "This function takes in a list or dictionary of Series or DataFrame objects and joins them along a particular axis, row-wise axis=0 or column-wise axis=1.\n",
    "\n",
    "```python\n",
    "# For example, concat with a list of two DataFrames\n",
    "pd.concat([df1, df2], axis=0)\n",
    "```\n",
    "\n",
    "- When your list contains at least one DataFrame, a DataFrame is returned.\n",
    "\n",
    "\n",
    "- When concatenating only Series objects row-wise, axis=0, a Series is returned.\n",
    "\n",
    "\n",
    "- When concatenating Series or DataFrames column-wise, axis=1, a DataFrame is returned.\n",
    "\n",
    "```python\n",
    "# Default is set to row-wise concatenation using an outer join.\n",
    "pd.concat(objs, axis=0, join='outer')\n",
    "```\n",
    "\n",
    "When concatenating dataframes vertically, we basically are just adding more rows to an existing dataframe. In this case, the dataframes we are putting together should have the same column names[^1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"con\" + \"cat\" + \"e\" + \"nation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'a': [1, 2, 3]})\n",
    "df2 = pd.DataFrame({'a': [4, 5, 6]})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the indices are preserved on the resulting dataframe; we could set the `ignore_index` parameter to `True` if we wanted these to be sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df1 = pd.concat([df1, df2], ignore_index=True)\n",
    "concat_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]:\n",
    "    We can concatenate dataframes with different column names, but generally this is not the behavior we want, as pandas will fill in a lot of null values into the resulting dataframe. The exception to this is if the dataframes are aligned on their index (i.e. the labels for each row), then we can provide the `axis=1` keyword argument to `pd.concat` to merge the dataframes horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df2 = pd.DataFrame({'b': [1, 2, 3, 4, 5, 6]})\n",
    "concat_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis=1 means concat horizontally\n",
    "pd.concat([concat_df1, concat_df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.merge`\n",
    "\n",
    "This method is similar to a SQL join. Here's a [cool read](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html#compare-with-sql-join) making a comparison between the two, if you're interested.\n",
    "\n",
    "```python\n",
    "# df.merge default settings for commonly used parameters.\n",
    "\n",
    "left_df.merge(right_df, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, indicator=False)\n",
    "```\n",
    "\n",
    "How does changing the default argument of the `how` parameter change my resulting DataFrame?\n",
    "\n",
    "`how` == Type of merge to be performed.\n",
    "\n",
    "`how=left`: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "\n",
    "`how=right`: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "\n",
    "`how=outer`: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "\n",
    "`how=inner`: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the users DataFrame.\n",
    "\n",
    "users = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5, 6],\n",
    "    'name': ['bob', 'joe', 'sally', 'adam', 'jane', 'mike'],\n",
    "    'role_id': [1, 2, 3, 3, np.nan, np.nan]\n",
    "})\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the roles DataFrame\n",
    "\n",
    "roles = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['admin', 'author', 'reviewer', 'commenter']\n",
    "})\n",
    "roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.merge` method will allow us to specify `left_on` and `right_on` to indicate the columns that are the keys used to merge the dataframes together. \n",
    "\n",
    "- In addition, the `how` keyword argument is used to define what type of JOIN we want to do; as we saw above, `inner` is the default setting. \n",
    "\n",
    "- For demonstration purposes, I'm also going to set the `indicator` parameter to `True`, which will create a column indicating whether the merge key appears in the `left_only`, `right_only` or `both` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an outer join specifying the left and right DataFrame keys.\n",
    "\n",
    "users.merge(roles, left_on='role_id', right_on='id', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have duplicate column names in the resulting dataframe. By default, pandas will add a suffix of `_x` to any columns in the left dataframe that are duplicated, and `_y` to any columns in the right dataframe that are duplicated. I can clean up my columns if I want to; one way would be to use method chaining, which it demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join tolerates no nulls.\n",
    "# Inner join only shows user who have roles, roles that have users\n",
    "users.merge(roles, how=\"inner\", left_on=\"role_id\", right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(users.merge(roles, \n",
    "            left_on='role_id', \n",
    "            right_on='id', \n",
    "            how='outer')\n",
    "    .drop(columns='role_id')\n",
    "    .rename(columns={'id_x': 'id', \n",
    "                     'name_x': 'employee',\n",
    "                     'id_y': 'role_id',\n",
    "                     'name_y': 'role'}\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises II\n",
    "\n",
    "1. Copy the `users` and `roles` DataFrames from the examples above. \n",
    "\n",
    "2. What is the result of using a `right` join on the DataFrames? \n",
    "\n",
    "3. What is the result of using an `outer` join on the DataFrames?\n",
    "     \n",
    "4. What happens if you drop the foreign keys from the DataFrames and try to merge them?\n",
    "\n",
    "5. Load the `mpg` dataset from PyDataset. \n",
    "\n",
    "6. Output and read the documentation for the `mpg` dataset.\n",
    "\n",
    "7. How many rows and columns are in the dataset?\n",
    "\n",
    "8. Check out your column names and perform any cleanup you may want on them.\n",
    "\n",
    "9. Display the summary statistics for the dataset.\n",
    "\n",
    "10. How many different manufacturers are there?\n",
    "\n",
    "11. How many different models are there?\n",
    "\n",
    "12. Create a column named `mileage_difference` like you did in the DataFrames exercises; this column should contain the difference between highway and city mileage for each car.\n",
    "\n",
    "13. Create a column named `average_mileage` like you did in the DataFrames exercises; this is the mean of the city and highway mileage.\n",
    "\n",
    "14. Create a new column on the `mpg` dataset named `is_automatic` that holds boolean values denoting whether the car has an automatic transmission.\n",
    "\n",
    "15. Using the `mpg` dataset, find out which which manufacturer has the best miles per gallon on average?\n",
    "\n",
    "16. Do automatic or manual cars have better miles per gallon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n",
    "\n",
    "We will talk about reshaping operations in more detail when we discuss tidy data, but for now we will focus on a couple of common operations that can be used to summarize our data by different subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.crosstab`\n",
    "\n",
    "For an example of `.crosstab`, we will count the number of students passing math in each classroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use our student grades DataFrame, df.\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `pd.crosstab` function to count the number of occurances of each subgroup (i.e. each unique combination of classroom and whether or not the student is passing math):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.passing_math, df.classroom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view subtotals with the `margins` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.passing_math, df.classroom, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.crosstab` function will let us view the numbers as percentages of the total as well by setting `normalize` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.passing_math, df.classroom, normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.pivot_table`\n",
    "\n",
    "Here we use the `.pivot_table` method to create our summary. This method produces output similar to an excel pivot table. We must supply 3 things here:\n",
    "\n",
    "- which values will make up the rows (the `index`)\n",
    "- which values will make up the columns\n",
    "- the values we are aggregating\n",
    "- an aggregation method (`aggfunc`); if we can omit this, and `mean` will be used by default\n",
    "\n",
    "For an example using the `pivot_table` method, we'll calculate the average math grade for the combination of `classroom` and `passing_math` status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='classroom', columns='passing_math', values='math')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create a dataframe that represents various orders at a restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'drink': np.random.choice(['Tea', 'Water', 'Water'], n),\n",
    "    'meal': np.random.choice(['Curry', 'Yakisoba Noodle', 'Pad Thai'], n),\n",
    "})\n",
    "\n",
    "orders.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.map`\n",
    "\n",
    "The `.map` method lets us use a dictionary to calculate the total price for an order; then I can save my calculations to a new column named `bill`. Let's do this step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of prices for drinks and meals.\n",
    "\n",
    "prices = {\n",
    "    'Yakisoba Noodle': 9,\n",
    "    'Curry': 11,\n",
    "    'Pad Thai': 10,\n",
    "    'Tea': 2,\n",
    "    'Water': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Match the values in the 'drink' and 'meal' columns with the values in the 'prices' dictionary \n",
    "and perform the specified calculation. Save this calculation to a new column named 'bill'.\n",
    "\"\"\"\n",
    "\n",
    "orders['bill'] = orders.drink.map(prices) + orders.meal.map(prices)\n",
    "\n",
    "orders.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how many orders have each combination of meal and drink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(orders.drink, orders.meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(orders.drink, orders.meal, normalize=True, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's find out the average bill amount for each combination: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.pivot_table(index='drink', columns='meal', values='bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that we could find the same information with a multi-level group by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.groupby(['drink', 'meal']).bill.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice between group by and a pivot table here is mostly asthetic, and you should use whichever makes more sense to you with the problem at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises III\n",
    "\n",
    "1. Use your `get_db_url` function to help you explore the data from the `chipotle` database. \n",
    "\n",
    "2. What is the total price for each order?\n",
    "\n",
    "3. What are the most popular 3 items?\n",
    "\n",
    "4. Which item has produced the most revenue?\n",
    "\n",
    "5. Join the `employees` and `titles` DataFrames together.\n",
    "\n",
    "6. For each title, find the hire date of the employee that was hired most recently with that title.\n",
    "\n",
    "7. Write the code necessary to create a cross tabulation of the number of titles by department. (Hint: this will involve a combination of SQL code to pull the necessary data and python/pandas code to perform the manipulations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Pandas Exercises and Resources\n",
    "- [https://www.w3resource.com/python-exercises/pandas/index.php](https://www.w3resource.com/python-exercises/pandas/index.php)\n",
    "- [https://towardsdatascience.com/20-pandas-functions-that-will-boost-your-data-analysis-process-f5dfdb2f9e05](https://towardsdatascience.com/20-pandas-functions-that-will-boost-your-data-analysis-process-f5dfdb2f9e05)\n",
    "- [https://github.com/guipsamora/pandas_exercises](https://github.com/guipsamora/pandas_exercises)\n",
    "- [https://github.com/ajcr/100-pandas-puzzles](https://github.com/ajcr/100-pandas-puzzles)\n",
    "\n",
    "### More Practice!\n",
    "\n",
    "For even more practice with pandas, you can do the exercises from the SQL module, but instead of using SQL to do the aggregation, sorting, joining, etc, use pandas. That is, read the data from all of the tables into pandas dataframes and manipulate the dataframes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
